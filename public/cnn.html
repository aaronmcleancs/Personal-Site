<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Project Showcase - CVV_15M_SARS-CoV-2</title>
    <link rel="stylesheet" href="css/styles.css">
    <link rel="stylesheet" href="css/showcase.css">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-okaidia.min.css" rel="stylesheet" />
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/4.1.1/animate.min.css"/>
    <script src="https://kit.fontawesome.com/ca7f2ffa51.js" crossorigin="anonymous"></script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <div class="navbar">
        <div class="logo">
            <div class="icon"><img src="images/blurIcon.png" height="27px" width="27px" alt="Logo"></div>
        </div>
        <a href="index.html">Home</a>
        <a href="#overview">Overview</a>
        <a href="#features">Features</a>
    </div>

    <div class="showcase__hero">
        <div class="showcase__wrapper">
            <h1 class="animate__animated animate__fadeIn">CVV_15M_SARS-CoV-2</h1>
            <p class="animate__animated animate__fadeIn animate__delay-1s">X-ray Classification Model built on 15 Million Parameter Convolutional Neural Network.</p>
        </div>
    </div>

    <div class="showcase__content">
        <section id="overview" class="showcase__section">
            <h2>Overview</h2>
            <p>This project implements a Convolutional Neural Network (CNN) to classify chest X-ray images and explores the undlerying archetchure of ReLu. Leveraging the TensorFlow and Keras frameworks, the model weights are optimized to run on Apple M-series CPUs, evaluated accuracy >97%.</p>
        </section>

        <section id="features" class="showcase__section">
            <h2>Features</h2>
            <ul>
                <li>High-accuracy classification of chest X-rays (>97% accuracy)</li>
                <li>TensorFlow, Keras, NumPy for model training and evaluation</li>
                <li>Optimized alexnet implementation for local deployment on Apple M1 Pro</li>
            </ul>

            <div class="code-sample">
                <pre><code class="language-python">
def create_model():
print("Creating a more complex model with approximately 15 million parameters...")
model = keras.Sequential([
    keras.layers.Conv2D(64, (3, 3), activation='relu', input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3)),
    keras.layers.MaxPooling2D((2, 2)),
    keras.layers.Conv2D(128, (3, 3), activation='relu'),
    keras.layers.MaxPooling2D((2, 2)),
    keras.layers.Conv2D(256, (3, 3), activation='relu'),
    keras.layers.MaxPooling2D((2, 2)),
    keras.layers.Conv2D(512, (3, 3), activation='relu'),
    keras.layers.MaxPooling2D((2, 2)),
    keras.layers.Flatten(),
    keras.layers.Dense(1024, activation='relu'),
    keras.layers.Dropout(0.5),
    keras.layers.Dense(1024, activation='relu'),
    keras.layers.Dropout(0.5),
    keras.layers.Dense(NUM_CLASSES, activation='softmax')
])
return model
                </code></pre>
            </div>
        </section>

        <section id="methodology" class="showcase__section">
            <h2>Methodology & Optimization</h2>
            
<p>
    AlexNet, introduced by Krizhevsky et al. in 2012 was pivotal in the field of deep learning. This convolutional neural network (CNN) outperformed models on the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) significantly and can be deployed in low weight densities.
</p>

<h4>ReLU Activation Function</h4>
<p>
    One of AlexNet's key innovations was the widespread use of the Rectified Linear Unit (ReLU) activation function replacing sigmoid learning functions. The ReLU function is defined as:
</p>
<p>
    \[ f(x) = \max(0, x) \]
</p>
<p>
    ReLU addresses the vanishing gradient problem in sigmoid functions, allowing for faster training. Evidence from widely used datasets such as CIFAR-10 demonstrated that deep convolutional neural networks with ReLUs reach a given accuracy up to six times faster than their equivalents with tanh units.
    Unlike sigmoid functions that saturate and kill gradients, ReLU allows a network to easily obtain representations. When a neuron's output is negative, it's set to zero and does not activate. This sparsity is theorized to be beneficial for neural networks, as it allows for more efficient and disentangled representations.
</p>

<h4>Local Response Normalization (LRN)</h4>
<p>
    AlexNet introduced Local Response Normalization, a technique inspired by lateral inhibition in biological neurons. LRN encourages competition among neuron outputs at the same spatial position but across feature maps. The normalized activity \( b_{i,x,y} \) of a neuron using the kernel \( i \) at position \( (x,y) \) is given by:
</p>
<p>
    \[
    b_{i,x,y} = \frac{a_{i,x,y}}{\left(k + \alpha \sum_{j=\max(0,i-n/2)}^{\min(N-1,i+n/2)} (a_{j,x,y})^2\right)^\beta}
    \]
</p>
<p>
    Where:
    <ul>
        <li>\( a_{i,x,y} \) is the activity of a neuron computed by applying kernel \( i \) at position \( (x,y) \)</li>
        <li>\( N \) is the total number of kernels in the layer</li>
        <li>\( n \) is the size of the normalization neighborhood</li>
        <li>\( k \), \( \alpha \), and \( \beta \) are hyperparameters</li>
    </ul>
</p>

<h4>Overlapping Pooling</h4>
<p>
    AlexNet employs overlapping max pooling, where the pooling regions overlap. This is characterized by stride \( s < z \), where \( z \) is the filter size. For instance, AlexNet uses \( z=3 \) and \( s=2 \). Overlapping pooling was found to marginally reduce classification error rates compared to non-overlapping pooling, and also helps to reduce overfitting.
</p>

<h4>Dropout Regularization</h4>
<p>
    To combat overfitting, AlexNet employs dropout with a rate of 0.5 in the first two fully connected layers. Dropout can be viewed as a form of model averaging, where for each training sample, a random subset of neurons is "dropped out" or temporarily removed from the network. The probability of including a hidden unit is given by:
</p>
<p>
    \[
    P(h_j|x) = \sum_i P(h_j|i)P(i|x)
    \]
</p>
<p>
    Where \( i \) indexes the possible subsets of hidden units, and \( P(i|x) \) is uniform over all \( 2^n \) subsets of hidden units.
</p>

<h4>Data Augmentation</h4>
<p>
    AlexNet employs two forms of data augmentation:
    <ul>
        <li>Image translations and horizontal reflections: The original 256x256 images are cropped to 224x224 patches at random positions during training.</li>
        <li>Altering RGB channel intensities: Principal Component Analysis (PCA) is performed on the set of RGB pixel values throughout the training set. The following quantity is added to each image pixel:
        </li>
    </ul>
</p>
<p>
    \[
    \mathbf{p}_1, \mathbf{p}_2, \mathbf{p}_3\alpha_1\lambda_1, \alpha_2\lambda_2, \alpha_3\lambda_3
    \]
</p>

<h4>Architecture Details</h4>
<p>
    AlexNet consists of eight learned layers - five convolutional and three fully-connected. The network's structure can be represented as:
</p>

<img src="/images/alexnet.png" alt="AlexNet Diagram" style="max-width: 1300px; width: 100%; height: auto; display: block; margin: 0 auto;">
<h4>Optimization and Training</h4>
<p>
    The model was trained using the Adam optimizer with an initial learning rate of 1e-3 on a dataset of n=4800 labelled X-rays. The update rule for the Adam optimizer can be summarized as:
<p>
    \[
    m_t = \beta_1 \cdot m_{t-1} + (1 - \beta_1) \cdot g_t
    \]
    \[
    v_t = \beta_2 \cdot v_{t-1} + (1 - \beta_2) \cdot g_t^2
    \]
    \[
    \hat{m}_t = \frac{m_t}{1 - \beta_1^t}
    \]
    \[
    \hat{v}_t = \frac{v_t}{1 - \beta_2^t}
    \]
    \[
    w_t = w_{t-1} - \frac{\eta}{\sqrt{\hat{v}_t} + \epsilon} \cdot \hat{m}_t
    \]
</p>
<p>
    Where \( m_t \) and \( v_t \) are the first and second moment estimates, \( g_t \) is the gradient at time \( t \), \( \beta_1 \) and \( \beta_2 \) are the exponential decay rates for the moment estimates, \( \eta \) is the learning rate, and \( \epsilon \) is a small constant for numerical stability.
</p>
<p>
    The model was compiled with categorical crossentropy as the loss function and accuracy as the metric. Early stopping was implemented with a patience of 5 epochs, restoring the best weights when triggered. Additionally, a custom TensorBoard callback was used to log the learning rate during training.
</p>
        </section>

        <section id="results" class="showcase__section">
            <h2>Results</h2>
            <div class="code-sample">
                <pre><code class="language-python">
Configuration:
IMAGE_SIZE: 224
BATCH_SIZE: 16
EPOCHS: 30
NUM_CLASSES: 3

Starting the COVID-19 X-ray Classification process...
Loading and preprocessing data...
Processing COVID images...
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3616/3616 [00:05<00:00, 629.91it/s]
Processing Normal images...
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5001/5001 [00:08<00:00, 573.78it/s]
Processing Viral Pneumonia images...
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1345/1345 [00:02<00:00, 544.96it/s]
Data loading complete.
Data shape: (9961, 224, 224, 3), Labels shape: (9961,)
Encoded labels shape: (9961, 3)
Train set: (6374, 224, 224, 3), Validation set: (1594, 224, 224, 3), Test set: (1993, 224, 224, 3)
Creating a more complex model with approximately 15 million parameters...
/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(activity_regularizer=activity_regularizer, **kwargs)
Model compiled. Summary:
Model: "sequential"
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓
┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩
│ conv2d (Conv2D)                      │ (None, 222, 222, 64)        │           1,792 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ max_pooling2d (MaxPooling2D)         │ (None, 111, 111, 64)        │               0 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ conv2d_1 (Conv2D)                    │ (None, 109, 109, 128)       │          73,856 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ max_pooling2d_1 (MaxPooling2D)       │ (None, 54, 54, 128)         │               0 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ conv2d_2 (Conv2D)                    │ (None, 52, 52, 256)         │         295,168 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ max_pooling2d_2 (MaxPooling2D)       │ (None, 26, 26, 256)         │               0 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ conv2d_3 (Conv2D)                    │ (None, 24, 24, 512)         │       1,180,160 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ max_pooling2d_3 (MaxPooling2D)       │ (None, 12, 12, 512)         │               0 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ flatten (Flatten)                    │ (None, 73728)               │               0 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ dense (Dense)                        │ (None, 1024)                │      75,498,496 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ dropout (Dropout)                    │ (None, 1024)                │               0 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ dense_1 (Dense)                      │ (None, 1024)                │       1,049,600 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ dropout_1 (Dropout)                  │ (None, 1024)                │               0 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ dense_2 (Dense)                      │ (None, 3)                   │           3,075 │
└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘
 Total params: 78,102,147 (297.94 MB)
 Trainable params: 78,102,147 (297.94 MB)
 Non-trainable params: 0 (0.00 B)
Starting model training...
Epoch 1/30
399/399 ━━━━━━━━━━━━━━━━━━━━ 401s 1s/step - accuracy: 0.5683 - loss: 1.4331 - val_accuracy: 0.5621 - val_loss: 1.1347 - learning_rate: 0.0010
Epoch 2/30
399/399 ━━━━━━━━━━━━━━━━━━━━ 391s 980ms/step - accuracy: 0.8538 - loss: 0.3874 - val_accuracy: 0.8193 - val_loss: 0.4369 - learning_rate: 0.0010
Epoch 3/30
399/399 ━━━━━━━━━━━━━━━━━━━━ 405s 1s/step - accuracy: 0.9046 - loss: 0.2606 - val_accuracy: 0.9159 - val_loss: 0.2156 - learning_rate: 0.0010
Epoch 4/30
399/399 ━━━━━━━━━━━━━━━━━━━━ 381s 954ms/step - accuracy: 0.9285 - loss: 0.1953 - val_accuracy: 0.9235 - val_loss: 0.2390 - learning_rate: 0.0010
Epoch 5/30
399/399 ━━━━━━━━━━━━━━━━━━━━ 393s 983ms/step - accuracy: 0.9396 - loss: 0.1712 - val_accuracy: 0.9241 - val_loss: 0.2734 - learning_rate: 0.0010
Epoch 6/30
399/399 ━━━━━━━━━━━━━━━━━━━━ 385s 961ms/step - accuracy: 0.9532 - loss: 0.1523 - val_accuracy: 0.9172 - val_loss: 0.2546 - learning_rate: 0.0010
Epoch 7/30
399/399 ━━━━━━━━━━━━━━━━━━━━ 345s 865ms/step - accuracy: 0.9606 - loss: 0.1082 - val_accuracy: 0.9442 - val_loss: 0.2148 - learning_rate: 0.0010
Epoch 8/30
399/399 ━━━━━━━━━━━━━━━━━━━━ 344s 861ms/step - accuracy: 0.9705 - loss: 0.0884 - val_accuracy: 0.9341 - val_loss: 0.2142 - learning_rate: 0.0010
Epoch 9/30
399/399 ━━━━━━━━━━━━━━━━━━━━ 358s 896ms/step - accuracy: 0.9679 - loss: 0.0869 - val_accuracy: 0.9241 - val_loss: 0.2759 - learning_rate: 0.0010
Epoch 10/30
399/399 ━━━━━━━━━━━━━━━━━━━━ 358s 898ms/step - accuracy: 0.9691 - loss: 0.0904 - val_accuracy: 0.9379 - val_loss: 0.2292 - learning_rate: 0.0010
Epoch 11/30
399/399 ━━━━━━━━━━━━━━━━━━━━ 379s 948ms/step - accuracy: 0.9759 - loss: 0.0687 - val_accuracy: 0.9354 - val_loss: 0.2390 - learning_rate: 0.0010
Epoch 12/30
399/399 ━━━━━━━━━━━━━━━━━━━━ 407s 1s/step - accuracy: 0.9802 - loss: 0.0579 - val_accuracy: 0.9404 - val_loss: 0.2453 - learning_rate: 0.0010
Epoch 13/30
399/399 ━━━━━━━━━━━━━━━━━━━━ 397s 994ms/step - accuracy: 0.9829 - loss: 0.0448 - val_accuracy: 0.9391 - val_loss: 0.3155 - learning_rate: 0.0010
Epoch 14/30
399/399 ━━━━━━━━━━━━━━━━━━━━ 378s 946ms/step - accuracy: 0.9840 - loss: 0.0481 - val_accuracy: 0.9435 - val_loss: 0.2474 - learning_rate: 0.0010
Epoch 15/30
399/399 ━━━━━━━━━━━━━━━━━━━━ 345s 865ms/step - accuracy: 0.9903 - loss: 0.0335 - val_accuracy: 0.9467 - val_loss: 0.2601 - learning_rate: 0.0010
Epoch 16/30
399/399 ━━━━━━━━━━━━━━━━━━━━ 344s 861ms/step - accuracy: 0.9917 - loss: 0.0252 - val_accuracy: 0.9391 - val_loss: 0.3377 - learning_rate: 0.0010
Epoch 17/30
399/399 ━━━━━━━━━━━━━━━━━━━━ 353s 884ms/step - accuracy: 0.9828 - loss: 0.0504 - val_accuracy: 0.9341 - val_loss: 0.3562 - learning_rate: 0.0010
Epoch 18/30
399/399 ━━━━━━━━━━━━━━━━━━━━ 344s 861ms/step - accuracy: 0.9835 - loss: 0.0651 - val_accuracy: 0.9348 - val_loss: 0.3991 - learning_rate: 0.0010
Epoch 19/30
399/399 ━━━━━━━━━━━━━━━━━━━━ 344s 861ms/step - accuracy: 0.9893 - loss: 0.0398 - val_accuracy: 0.9479 - val_loss: 0.2701 - learning_rate: 0.0010
Epoch 20/30
399/399 ━━━━━━━━━━━━━━━━━━━━ 355s 888ms/step - accuracy: 0.9764 - loss: 0.0829 - val_accuracy: 0.9373 - val_loss: 0.3054 - learning_rate: 0.0010
Epoch 21/30
399/399 ━━━━━━━━━━━━━━━━━━━━ 367s 919ms/step - accuracy: 0.9868 - loss: 0.0422 - val_accuracy: 0.9366 - val_loss: 0.2989 - learning_rate: 0.0010
Epoch 22/30
399/399 ━━━━━━━━━━━━━━━━━━━━ 383s 959ms/step - accuracy: 0.9901 - loss: 0.0295 - val_accuracy: 0.9348 - val_loss: 0.3332 - learning_rate: 0.0010
Epoch 23/30
399/399 ━━━━━━━━━━━━━━━━━━━━ 396s 990ms/step - accuracy: 0.9923 - loss: 0.0272 - val_accuracy: 0.9448 - val_loss: 0.2718 - learning_rate: 0.0010
Epoch 24/30
399/399 ━━━━━━━━━━━━━━━━━━━━ 400s 1s/step - accuracy: 0.9929 - loss: 0.0293 - val_accuracy: 0.9404 - val_loss: 0.3556 - learning_rate: 0.0010
Epoch 25/30
399/399 ━━━━━━━━━━━━━━━━━━━━ 397s 993ms/step - accuracy: 0.9918 - loss: 0.0331 - val_accuracy: 0.9373 - val_loss: 0.3701 - learning_rate: 0.0010
Epoch 26/30
399/399 ━━━━━━━━━━━━━━━━━━━━ 418s 1s/step - accuracy: 0.9936 - loss: 0.0278 - val_accuracy: 0.9228 - val_loss: 0.4954 - learning_rate: 0.0010
Epoch 27/30
399/399 ━━━━━━━━━━━━━━━━━━━━ 392s 982ms/step - accuracy: 0.9885 - loss: 0.0527 - val_accuracy: 0.9404 - val_loss: 0.3680 - learning_rate: 0.0010
Epoch 28/30
399/399 ━━━━━━━━━━━━━━━━━━━━ 382s 956ms/step - accuracy: 0.9914 - loss: 0.0350 - val_accuracy: 0.9423 - val_loss: 0.3616 - learning_rate: 0.0010
Epoch 29/30
399/399 ━━━━━━━━━━━━━━━━━━━━ 396s 991ms/step - accuracy: 0.9950 - loss: 0.0260 - val_accuracy: 0.9335 - val_loss: 0.4061 - learning_rate: 0.0010
Epoch 30/30
399/399 ━━━━━━━━━━━━━━━━━━━━ 365s 916ms/step - accuracy: 0.9859 - loss: 0.0629 - val_accuracy: 0.9373 - val_loss: 0.4053 - learning_rate: 0.0010
Evaluating model on test set...
63/63 - 36s - 569ms/step - accuracy: 0.9403 - loss: 0.3454
Test accuracy: 0.9403
WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
Model saved as 'chest_xray_classifier_final.h5'
Training history plot saved as 'training_history.png'
63/63 ━━━━━━━━━━━━━━━━━━━━ 36s 564ms/step
Confusion Matrix:
[[708  10   6]
 [ 70 922   8]
 [ 11  14 244]]

Classification Report:
              precision    recall  f1-score   support

       covid       0.90      0.98      0.94       724
      normal       0.97      0.92      0.95      1000
   pneumonia       0.95      0.91      0.93       269

    accuracy                           0.94      1993
   macro avg       0.94      0.94      0.94      1993
weighted avg       0.94      0.94      0.94      1993
                </code></pre>
            </div>
            <p>The model achieved impressive accuracy with balanced precision and recall across all target classes. The use of dropout and ReLU activations enhances the generalization capacity of the model, making it robust against overfitting.</p>
        
            <h3>Model Optimization for the M1 Pro 16GB RAM</h3>
            <p>Optimizing the model for the Apple M1 Pro with 16GB RAM involves several considerations given the unique architecture of the M1 series. The Apple M1 Pro combines 10 cores (8 performance and 2 efficiency cores) with an Apple M1 Pro GPU (16-core) integrated into a 5 nm fabrication process. This architecture, coupled with a base frequency of 3.2 GHz, offers robust capability for model processing.</p>
        
            <p>Key Focus Areas for Optimization:</p>
            <ul>
                <li><b>Memory Management:</b> With 16GB of unified memory, sizing the model correctly was essential.</li>
                <li><b>Computational Efficiency:</b>Optimized for Apple silicon with TensorFlow Metal plugin, offloading to the GPU.</li>
            </ul>
        </section>

        <section id="tech-stack" class="showcase__section">
            <h2>Stack</h2>
            <div class="tech-icons">
                <i class="fab fa-python"></i>
                <i class="fab fa-aws"></i>
                <i class="fab fa-kaggle"></i>
            </div>
        </section>

        <section id="gallery" class="showcase__section">
            <h2>Project Gallery</h2>
            <div class="image-gallery">
                <img src="images/cnn1.png" alt="Project screenshot 1">
                <img src="images/cnn2.png" alt="Project screenshot 2">
            </div>
        </section>
    </div>

    <div class="showcase__cta">
        <a href="https://github.com/aaronmcleancs/CVV_15M_SARS-CoV-2" class="button">View on GitHub</a>
        <a href="#" class="button">Live Demo</a>
    </div>
    <div style="text-align: center; margin-top: 20px; margin-bottom: 20px;">
        <a href="https://opensource.org/licenses/MIT" style="color: rgb(113, 113, 113); font-size: small;" target="_blank;">
            @Aaron McLean
        </a>
    </div>
    <div class="fullscreen-viewer">
        <img src="" alt="Fullscreen image" class="fullscreen-image">
        <div class="fullscreen-nav">
            <button class="fullscreen-prev">&lt;</button>
            <button class="fullscreen-next">&gt;</button>
        </div>
        <button class="fullscreen-close">&times;</button>
    </div>

    <script>
        const imageGallery = document.querySelector('.image-gallery');
        const fullscreenViewer = document.querySelector('.fullscreen-viewer');
        const fullscreenImage = document.querySelector('.fullscreen-image');
        const prevButton = document.querySelector('.fullscreen-prev');
        const nextButton = document.querySelector('.fullscreen-next');
        const closeButton = document.querySelector('.fullscreen-close');
        let currentImageIndex = 0;
        const images = Array.from(imageGallery.querySelectorAll('img'));

        function openFullscreen(index) {
            currentImageIndex = index;
            fullscreenImage.src = images[index].src;
            fullscreenViewer.style.display = 'flex';
            document.body.style.overflow = 'hidden';
        }

        function closeFullscreen() {
            fullscreenViewer.style.display = 'none';
            document.body.style.overflow = 'auto';
        }

        function showNextImage() {
            currentImageIndex = (currentImageIndex + 1) % images.length;
            fullscreenImage.src = images[currentImageIndex].src;
        }

        function showPrevImage() {
            currentImageIndex = (currentImageIndex - 1 + images.length) % images.length;
            fullscreenImage.src = images[currentImageIndex].src;
        }

        images.forEach((img, index) => {
            img.addEventListener('click', () => openFullscreen(index));
        });

        closeButton.addEventListener('click', closeFullscreen);
        nextButton.addEventListener('click', showNextImage);
        prevButton.addEventListener('click', showPrevImage);

        fullscreenViewer.addEventListener('click', (e) => {
            if (e.target === fullscreenViewer) {
                closeFullscreen();
            }
        });

        document.addEventListener('keydown', (e) => {
            if (fullscreenViewer.style.display === 'flex') {
                if (e.key === 'Escape') closeFullscreen();
                if (e.key === 'ArrowRight') showNextImage();
                if (e.key === 'ArrowLeft') showPrevImage();
            }
        });
    </script>
    <script src="js/showcase.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
</body>
</html>